{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annoy in /Users/mrpapa/miniforge3/envs/nlp/lib/python3.8/site-packages (1.17.2)\n",
      "Requirement already satisfied: efficient-apriori in /Users/mrpapa/miniforge3/envs/nlp/lib/python3.8/site-packages (2.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install annoy\n",
    "!pip install efficient-apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: art in /Users/mrpapa/miniforge3/envs/nlp/lib/python3.8/site-packages (5.9)\n",
      "Requirement already satisfied: plotly in /Users/mrpapa/miniforge3/envs/nlp/lib/python3.8/site-packages (5.14.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/mrpapa/miniforge3/envs/nlp/lib/python3.8/site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: packaging in /Users/mrpapa/miniforge3/envs/nlp/lib/python3.8/site-packages (from plotly) (23.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install art\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mrpapa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import flatten\n",
    "from annoy import AnnoyIndex\n",
    "from gensim.models import Word2Vec\n",
    "from efficient_apriori import apriori\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import zipfile as zp\n",
    "from art import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "from plotly.tools import FigureFactory as FF\n",
    "import plotly.express as px\n",
    "\n",
    "# from PyDictionary import PyDictionary P\n",
    "import random\n",
    "import time\n",
    "\n",
    "#import scikitplot as skplt\n",
    "\n",
    "#to enable the inline plotting\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the days and hours from numbers to their interpretable form\n",
    "import datetime\n",
    "days_of_week = {0: 'Saturday', \n",
    "                1: 'Sunday', \n",
    "                2: 'Monday',\n",
    "                3: 'Tuesday',\n",
    "                4: 'Wednesday',\n",
    "                5: 'Thursday',\n",
    "                6: 'Friday'}\n",
    "hour_nums = list(range(24))\n",
    "hours_of_day = {hour_num:datetime.time(hour_num).strftime(\"%I:00 %p\") for hour_num in hour_nums}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Instacart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to location of data (Instacart Market Basket Analysis Kaggle data)\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv files into corresponding dataframes\n",
    "# Load products, aisles and departments\n",
    "products = pd.read_csv(os.path.join(data_dir, \"products.csv\"))\n",
    "aisles = pd.read_csv(os.path.join(data_dir, \"aisles.csv\"))\n",
    "departments = pd.read_csv(os.path.join(data_dir, \"departments.csv\"))\n",
    "\n",
    "# Load orders dataset\n",
    "orders = pd.read_csv(os.path.join(data_dir, \"orders.csv\"))\n",
    "order_products_prior = pd.read_csv(os.path.join(data_dir, \"order_products__prior.csv\"))\n",
    "order_products_train = pd.read_csv(os.path.join(data_dir, \"order_products__train.csv\"))\n",
    "\n",
    "# Replacing numbers with their corresponding hour representation\n",
    "# orders['order_hour_of_day'] = orders['order_hour_of_day'].replace(to_replace=hours_of_day)\n",
    "# orders['order_hour_of_day'] = pd.Categorical(orders['order_hour_of_day'], \n",
    "#                                              ordered=True, \n",
    "#                                              categories=list(hours_of_day.values()))\n",
    "\n",
    "# Replacing numbers with their corresponding day of week\n",
    "# Define the categories of days of week sorted normally from Saturday to Friday\n",
    "orders['order_dow'] = orders['order_dow'].replace(to_replace=days_of_week)\n",
    "orders['order_dow'] = pd.Categorical(orders['order_dow'], \n",
    "                                     ordered=True, \n",
    "                                     categories=list(days_of_week.values()))\n",
    "\n",
    "orders['daytime'] = orders['order_dow'].astype('str') + orders['order_hour_of_day'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum order value (+1 for limit)\n",
    "MAX_ORDER_LIMIT = orders.order_id.max() + 1\n",
    "# Limiting the number of orders to process\n",
    "orders_limit = MAX_ORDER_LIMIT # use 10000/100000 for limits or MAX ORDER\n",
    "# Number of orders/baskets to pull similar to the requested\n",
    "orders_returns = 15\n",
    "# Number of dimensions of the vector annoy is going to store. \n",
    "vector_size = 64\n",
    "# Number of trees for queries. When making a query the more trees the easier it is to go down the right path. \n",
    "trees = 10\n",
    "# Number of product recommendation as maximum\n",
    "NUMBER_OUTPUT_PRODUCTS = 10\n",
    "\n",
    "# Sample size for the TSNE model and plot\n",
    "tsne_size = 1000\n",
    "# Threshold for a minimum support\n",
    "threshold = 1e-3\n",
    "# Threshold for the maximun number of products to bring\n",
    "threshold_top = 10\n",
    "# Threshold for distance, based on the quantile calculation of the basket distances\n",
    "threshold_distance= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['products_mod'] = products['product_name'].str.lower()\n",
    "# Clean special characters.\n",
    "products['products_mod'] = products['products_mod'].str.replace('\\W', ' ', regex=True)\n",
    "# Split products into terms: Tokenize.\n",
    "products['products_mod'] = products['products_mod'].str.split()\n",
    "# Merge the department and aisle names into the dataframe. \n",
    "products = pd.merge(products, departments, on=\"department_id\", how='outer')\n",
    "products = pd.merge(products, aisles, on=\"aisle_id\", how='outer')\n",
    "# Remove synonyms here in the list\n",
    "products['products_mod'] = products[['products_mod', 'aisle', 'department']].values.tolist()\n",
    "products['products_mod'] = products['products_mod'].apply(lambda x:list(flatten(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steam and lemmatisation of the product name\n",
    "# https://stackoverflow.com/a/25082458/3780957\n",
    "# https://en.wikipedia.org/wiki/Lemmatisation\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "products['products_lemma'] = products['products_mod'].apply(lambda row:[lemma.lemmatize(item) for item in row])\n",
    "products['products_lemma'] = products['products_lemma'].apply(lambda row:[sno.stem(item) for item in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49688it [00:01, 31370.70it/s]\n"
     ]
    }
   ],
   "source": [
    "### Training the `Word2Vec` model based on product lemma\n",
    "\n",
    "# Defining the maximun window\n",
    "window_max = max(products['products_lemma'].apply(lambda x:len(x)))\n",
    "\n",
    "# size=20: In order to make `Word2Vec` a little bit quicker and for memory efficiency we're going to use 20 dimensions.\n",
    "# window=49: In order to make sure all words are used in training the model, we're going to set a large.\n",
    "w2vec_model = Word2Vec(list(products['products_lemma']), vector_size=vector_size, window=window_max, min_count=1, workers=-1)\n",
    "\n",
    "### Vector calculation for products\n",
    "# Loop through each product and obtain the average of each string that makes a product. <br>\n",
    "# This will be the vector representation of the product. <br>\n",
    "# The vector representation of the product will be used to calculate the similarity between products. <br>\n",
    "# The similarity between products will be used to recommend products to the user. <br>\n",
    "\n",
    "# Loop through each word in the product name to generate the vector.\n",
    "prods_w2v = dict()\n",
    "for row, product in tqdm(products.iterrows()):\n",
    "    word_vector = list()\n",
    "    for word in product['products_lemma']:\n",
    "        word_vector.append(w2vec_model.wv[word])\n",
    "\n",
    "    prods_w2v[product['product_id']] = np.average(word_vector, axis=0)\n",
    "\n",
    "# Save vector values in list form to the dataframe.\n",
    "# products['vectors'] = prods_w2v.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.1 s, sys: 8.11 s, total: 46.2 s\n",
      "Wall time: 49.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Training the `Word2Vec` model based on cart\n",
    "# products which are ordered together will be closer in vector space\n",
    "\n",
    "# Get orders dataset to extract cart data\n",
    "orders_filter = order_products_prior[order_products_prior.order_id < orders_limit]\n",
    "order_baskets = orders_filter.groupby('order_id')['product_id'].apply(list)\n",
    "\n",
    "# Define maximum window for longest order cart\n",
    "window_max = order_baskets.apply(len).max()\n",
    "\n",
    "# w2vec model\n",
    "w2vec_model = Word2Vec(list(order_baskets), vector_size=vector_size, window=window_max, min_count=1, workers=-1)\n",
    "\n",
    "# get vectors for each product\n",
    "products['vectors'] = products.product_id.apply(lambda x: w2vec_model.wv[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Approximate Nearest Neighbours (using ANNOY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using `annoy` model to calculate the similarity between products\n",
    "# The `annoy` model is a library to search for points in space that are close to a given query point. <br>\n",
    "# It also creates large read-only file-based data structures that are mmpped into memory so that many processes may share the same data. <br>\n",
    "# For our case, we will use the `annoy` model to calculate the similarity between products. <br>\n",
    "# The `annoy` model is trained by taking as input a matrix of pairwise similarities between objects and converting them into probabilities using a Gaussian kernel. <br>\n",
    "# It then tries to minimize the Kullback–Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. <br>\n",
    "\n",
    "def annoy_build(df, id, metric='euclidean'):\n",
    "    m = AnnoyIndex(vector_size, metric=metric) \n",
    "    m.set_seed(42)\n",
    "    for _, row in df.iterrows():\n",
    "        m.add_item(row[id], row['vectors'])\n",
    "    m.build(trees)\n",
    "    return m\n",
    "\n",
    "def build_hnsw(df, id ,metric='euclidean'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.79 s, sys: 324 ms, total: 5.11 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Train `annoy` for `product` dataset\n",
    "# We need to specify ahead of time to annoy that there are 20 vector dimensions. Defined as a constant at `vector_size`.\n",
    "# We also specify we want the model to find distances using `euclidean` distance.\n",
    "\n",
    "# Specify the metric to be used for computing distances. \n",
    "p = annoy_build(products, 'product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3214874it [01:47, 29911.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 58s, sys: 13.4 s, total: 4min 11s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Train `annoy` for `orders` dataset\n",
    "order_w2v = dict()\n",
    "for index, row in tqdm(order_baskets.items()):\n",
    "    word_vector = list()\n",
    "    for item_id in row:\n",
    "        word_vector.append(p.get_item_vector(item_id))\n",
    "    order_w2v[index] = np.average(word_vector, axis=0)\n",
    "\n",
    "df_order_baskets = pd.DataFrame({'order_id': order_baskets.index, 'product_id': order_baskets.values})\n",
    "df_order_baskets['vectors'] = order_w2v.values()\n",
    "\n",
    "# Specify the metric to be used for computing distances. \n",
    "b = annoy_build(df_order_baskets, 'order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.8 s, sys: 1min 39s, total: 2min 22s\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Train `annoy` for `user` dataset\n",
    "# Creating an `annoy` object to index the `user` information\n",
    "user_basket = pd.merge(df_order_baskets, orders, on=\"order_id\", how='inner')\n",
    "user_basket = user_basket.groupby('user_id').apply(lambda x: [list(x['vectors']), list(x['product_id'])]).apply(pd.Series)\n",
    "user_basket.columns =['vectors','product_id']\n",
    "user_basket['vectors'] = user_basket['vectors'].apply(lambda x: tuple(np.average(x, axis=0)))\n",
    "user_basket['product_id'] = user_basket['product_id'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "user_basket['product_id'] = user_basket['product_id'].apply(lambda x: list(set(x)))\n",
    "df_user_basket = user_basket.reset_index()\n",
    "\n",
    "# Specify the metric to be used for computing distances. \n",
    "u = annoy_build(df_user_basket, 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.52 s, sys: 32.7 s, total: 42.2 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### Train `annoy` for `daytime` data\n",
    "daytime_basket = pd.merge(df_order_baskets, orders, on='order_id', how='inner')\n",
    "daytime_basket = daytime_basket.groupby('daytime').apply(lambda x: [list(x['vectors']), list(x['product_id'])]).apply(pd.Series)\n",
    "daytime_basket.columns =['vectors','product_id']\n",
    "daytime_basket['vectors'] = daytime_basket['vectors'].apply(lambda x: tuple(np.average(x, axis=0)))\n",
    "daytime_basket['vectors_list'] = daytime_basket['vectors'].apply(list)\n",
    "daytime_basket['product_id'] = daytime_basket['product_id'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "daytime_basket['product_id'] = daytime_basket['product_id'].apply(lambda x: list(set(x)))\n",
    "df_daytime_basket = daytime_basket.reset_index().reset_index().rename(columns={'index':'daytime_id'})\n",
    "# Specify the metric to be used for computing distances. \n",
    "d = annoy_build(df_daytime_basket, 'daytime_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Daytime embeddings using tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "# import plotly.express as px\n",
    "\n",
    "# daytime_index = orders[['order_dow', 'order_hour_of_day', 'daytime']].drop_duplicates().sort_values('order_dow').reset_index(drop = True).reset_index().rename(columns={'index':'daytime_id'})\n",
    "\n",
    "# day_shape_mapping = {\n",
    "#     'Monday': 'circle',\n",
    "#     'Tuesday': 'square',\n",
    "#     'Wednesday': 'diamond',\n",
    "#     'Thursday': 'cross',\n",
    "#     'Friday': 'star',\n",
    "#     'Saturday': 'triangle-up',\n",
    "#     'Sunday': 'triangle-down'\n",
    "# }\n",
    "\n",
    "# # Extract labels and vectors\n",
    "# labels = df_daytime_basket['daytime']\n",
    "# vectors = np.array(daytime_basket['vectors'].tolist())\n",
    "\n",
    "# # Apply t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# embedded_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# # Create a DataFrame for the embedded vectors\n",
    "# embedded_df = pd.DataFrame(embedded_vectors, columns=['Dimension 1', 'Dimension 2'])\n",
    "# embedded_df['daytime'] = labels\n",
    "# embedded_df = embedded_df.merge(daytime_index, on='daytime')\n",
    "# embedded_df['Shape'] = embedded_df['order_dow'].apply(lambda x: day_shape_mapping[x])\n",
    "\n",
    "# # Create an interactive scatter plot with Plotly\n",
    "# fig = px.scatter(embedded_df, x='Dimension 1', y='Dimension 2', color='order_hour_of_day', symbol='order_dow', hover_data=['daytime'])\n",
    "\n",
    "# # Update plot title and axis labels\n",
    "# fig.update_layout(\n",
    "#     title='t-SNE Visualization',\n",
    "#     xaxis_title='Dimension 1',\n",
    "#     yaxis_title='Dimension 2',\n",
    "#     showlegend=True,\n",
    "#     coloraxis_colorbar=dict(yanchor=\"top\", y=1, x=0,\n",
    "#                                           ticks=\"outside\")\n",
    "# )\n",
    "\n",
    "# fig.write_html(\"daytime_profile.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule Mining to get best recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarity between products\n",
    "### Define the function to calculate the similarity between products\n",
    "\n",
    "# List the unique products maintaining the original order\n",
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "# Sort recommendations by `lift`, and filter if the products are too close\n",
    "def product_lift(basket, input = None, order_baskets=order_baskets, th_support=threshold, th_n=threshold_top, products=products):\n",
    "    # Force to include the manual `input`\n",
    "    recommendations = basket['product_id'].tolist()\n",
    "    if input is not None:\n",
    "        recommendations.extend(input)\n",
    "    recommendations = set(recommendations)\n",
    "    \n",
    "    # Get all instances where either 1 or many products in recommendations were ordered together\n",
    "    # Identify all orders where atleast 1 recommended product is available\n",
    "    df_ = order_baskets[order_baskets.apply(lambda x: any(i in recommendations for i in x))].tolist()\n",
    "    # For each order cart, only keep recommended products in cart\n",
    "    order_baskets_ = [[i for i in sublist if i in recommendations] for sublist in df_]\n",
    "\n",
    "    # Calculate `apriori` rules using a efficient library to speed up the calculation\n",
    "    _, rules = apriori(order_baskets_, min_support=th_support, min_confidence=1e-2, max_length=5)\n",
    "    \n",
    "    # Multiple filters, but due to the lack of orders, are limiting the number of results, so a simple filter is active\n",
    "    if input is not None:\n",
    "        rules_rhs = filter(lambda rule: \\\n",
    "            not all(x in rule.rhs for x in input)\n",
    "            , rules)\n",
    "    else:\n",
    "        rules_rhs = rules\n",
    "\n",
    "    # Combine all the rules found in the data\n",
    "    # Sorted by highest lift\n",
    "    rule_combined = list()\n",
    "    for rule in sorted(rules_rhs, key=lambda rule: rule.lift, reverse=True):\n",
    "        # print(rule)\n",
    "        rule_combined.extend(rule.rhs)\n",
    "\n",
    "    # List the unique products maintaining the original order\n",
    "    product_recommendation = unique_preserve_order(rule_combined)\n",
    "\n",
    "    ## The following code, filters the recommendations after `lift`, based on the distance between the products\n",
    "    # List of products\n",
    "    prod = pd.DataFrame({'product_id': product_recommendation})\n",
    "    prod_cross_join = prod.merge(prod, how='cross')\n",
    "    # Calculate the distance between all the products\n",
    "    prod_cross_join['distance'] = prod_cross_join.apply(lambda row: p.get_distance(row['product_id_x'], row['product_id_y']), axis=1)\n",
    "    # Remove the same product (distance==0)\n",
    "    prod_cross_join = prod_cross_join[prod_cross_join['distance']!=0]\n",
    "    prod_cross_join.sort_values('distance', ascending=False)\n",
    "    # Looking for closest products\n",
    "    # Threshold for the filter, 10% of the distance (defined at `threshold_distance` constant)\n",
    "    th_distance = np.quantile(prod_cross_join, threshold_distance)\n",
    "    for id in product_recommendation:\n",
    "        to_be_removed = prod_cross_join.loc[(prod_cross_join['product_id_x']==id) & (prod_cross_join['distance']<th_distance), 'product_id_y']\n",
    "        prod_cross_join = prod_cross_join[~prod_cross_join['product_id_x'].isin(to_be_removed)]\n",
    "    # List of final recommendations after the filters and thresholds\n",
    "    prod_after_filtered = prod_cross_join['product_id_x'].unique()\n",
    "    # Retain the order from the `lift`\n",
    "    product_recommendation_filtered = pd.DataFrame({'product_recommendation': product_recommendation}).set_index('product_recommendation').loc[prod_after_filtered].reset_index()\n",
    "    # Recall the products in the previous order\n",
    "    product_recommendation_product = products.set_index(\"product_id\").loc[product_recommendation_filtered['product_recommendation']].reset_index()\n",
    "\n",
    "    return product_recommendation_product[['product_name', 'department', 'aisle']].head(th_n)\n",
    "\n",
    "# Finds the recommended basket, based on the `Word2Vec` vector as input\n",
    "def basket_recompose(w2v, b=b, order_baskets=order_baskets):\n",
    "    # Search for a similar basket in `b`\n",
    "    similar_baskets = b.get_nns_by_vector(w2v, orders_returns, search_k=-1, include_distances=False)\n",
    "    basket_recompose = pd.DataFrame({'order_id': similar_baskets, 'product_id': order_baskets[similar_baskets].values}).explode('product_id')\n",
    "\n",
    "    return basket_recompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_recommendation(input_vector):\n",
    "    product_list = p.get_nns_by_vector(input_vector, n=15)\n",
    "    return products[products.product_id.isin(product_list)][['product_name', 'department', 'aisle']].reset_index(drop=True)\n",
    "    \n",
    "def filter_dt_recommendation(x_product = [], x_user = [], daytime_id = None):\n",
    "\n",
    "    input = None\n",
    "    user_basket = None\n",
    "    product_basket = None\n",
    "    final_vector_list = list()\n",
    "    \n",
    "    basket = pd.DataFrame()\n",
    "    if x_user:\n",
    "        word_vector = list()\n",
    "        for user in x_user:\n",
    "            word_vector.append(tuple(u.get_item_vector(user)))\n",
    "        user_w2v = np.average(word_vector, axis=0)\n",
    "        final_vector_list.append(user_w2v)\n",
    "        \n",
    "        user_basket = basket_recompose(user_w2v)\n",
    "        basket = pd.concat([basket, user_basket], axis=0)\n",
    "\n",
    "    if x_product:\n",
    "        word_vector = list()\n",
    "        for item_id in x_product:\n",
    "            word_vector.append(p.get_item_vector(item_id))\n",
    "        product_w2v = np.average(word_vector, axis=0)\n",
    "        final_vector_list.append(product_w2v)\n",
    "        \n",
    "        similar_products = p.get_nns_by_vector(product_w2v, 100 + len(x_product), search_k=-1, include_distances=False)\n",
    "        product_basket = pd.DataFrame({'order_id': 0, 'product_id': similar_products})\n",
    "        product_basket = product_basket[~product_basket['product_id'].isin(x_product)]\n",
    "        basket = pd.concat([basket, product_basket], axis=0)\n",
    "        input = x_product\n",
    "\n",
    "    basket = basket.reset_index(drop=True).drop_duplicates('product_id')\n",
    "\n",
    "    # If daytime is available, filter those products which were ever sold in that daytime + 4 similar daytimes\n",
    "    if daytime_id is not None:\n",
    "        DAYTIME_NEIGHBOURS = 10\n",
    "        similar_daytime = d.get_nns_by_item(daytime_id, n=DAYTIME_NEIGHBOURS)\n",
    "        filter_list_of_list = df_daytime_basket[df_daytime_basket.daytime_id.isin(similar_daytime)]['product_id'].tolist()\n",
    "        filter_list = list(set([i for sublist in filter_list_of_list for i in sublist]))\n",
    "        basket = basket[basket.product_id.isin(filter_list)]\n",
    "\n",
    "    if len(final_vector_list) > 1:\n",
    "        final_vector = np.average(final_vector_list, axis=0)\n",
    "    else:\n",
    "        final_vector = final_vector_list[0]\n",
    "    \n",
    "    try:\n",
    "        return product_lift(basket, input), user_basket, product_basket, basket, final_vector\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return get_simple_recommendation(final_vector), user_basket, product_basket, basket, final_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XL Pick-A-Size Paper Towel Rolls\n",
      "Zero Calorie Cola\n",
      "CPU times: user 9.54 s, sys: 34.6 s, total: 44.1 s\n",
      "Wall time: 51.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>department</th>\n",
       "      <th>aisle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Soda</td>\n",
       "      <td>beverages</td>\n",
       "      <td>soft drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero Calorie Cola</td>\n",
       "      <td>beverages</td>\n",
       "      <td>soft drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Strawberries</td>\n",
       "      <td>produce</td>\n",
       "      <td>fresh fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Organic Whole String Cheese</td>\n",
       "      <td>dairy eggs</td>\n",
       "      <td>packaged cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Organic Strawberries</td>\n",
       "      <td>produce</td>\n",
       "      <td>fresh fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Organic Whole Milk</td>\n",
       "      <td>dairy eggs</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No Salt Added Black Beans</td>\n",
       "      <td>canned goods</td>\n",
       "      <td>canned meals beans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Organic Large Extra Fancy Fuji Apple</td>\n",
       "      <td>produce</td>\n",
       "      <td>fresh fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shredded Parmesan</td>\n",
       "      <td>dairy eggs</td>\n",
       "      <td>packaged cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Organic Turkey Bacon</td>\n",
       "      <td>meat seafood</td>\n",
       "      <td>hot dogs bacon sausage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           product_name    department                   aisle\n",
       "0                                  Soda     beverages             soft drinks\n",
       "1                     Zero Calorie Cola     beverages             soft drinks\n",
       "2                          Strawberries       produce            fresh fruits\n",
       "3           Organic Whole String Cheese    dairy eggs         packaged cheese\n",
       "4                  Organic Strawberries       produce            fresh fruits\n",
       "5                    Organic Whole Milk    dairy eggs                    milk\n",
       "6             No Salt Added Black Beans  canned goods      canned meals beans\n",
       "7  Organic Large Extra Fancy Fuji Apple       produce            fresh fruits\n",
       "8                     Shredded Parmesan    dairy eggs         packaged cheese\n",
       "9                  Organic Turkey Bacon  meat seafood  hot dogs bacon sausage"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# basket = filter_dt_recommendation(x_product = [26405, 46149], x_user = [1], daytime_id=165)\n",
    "df, userb, prodb, basket, input_vector = filter_dt_recommendation(x_product = [26405, 46149], x_user = [1], daytime_id=165)\n",
    "for prod in [26405, 46149]:\n",
    "    print(products[products.product_id == prod]['product_name'].item())\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.76 ms, sys: 1.12 s, total: 1.13 s\n",
      "Wall time: 6.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### This section will save resources\n",
    "### These resources can later be used by an app to run the engine\n",
    "\n",
    "save_path = os.path.join(os.path.dirname(os.getcwd()), 'res')\n",
    "\n",
    "def save_annoy(obj, n):\n",
    "    path = os.path.join(save_path, n + \".ann\")\n",
    "    obj.save(path)\n",
    "\n",
    "## Save annoy objects\n",
    "save_annoy(p, \"product\")\n",
    "save_annoy(u, \"user\")\n",
    "save_annoy(b, \"basket\")\n",
    "save_annoy(d, \"daytime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mrpapa/upwork/nlp/res/products.pkl saved !\n",
      "/Users/mrpapa/upwork/nlp/res/order_baskets.pkl saved !\n",
      "/Users/mrpapa/upwork/nlp/res/df_daytime_basket.pkl saved !\n",
      "CPU times: user 1.57 s, sys: 3.25 s, total: 4.82 s\n",
      "Wall time: 5.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Save dataframes to avoid pre-processing\n",
    "def save_dataframe(obj, n):\n",
    "    path = os.path.join(save_path, n + \".pkl\")\n",
    "    obj.to_pickle(path)\n",
    "    print(path, \"saved !\")\n",
    "\n",
    "save_dataframe(products, 'products')\n",
    "save_dataframe(order_baskets, 'order_baskets')\n",
    "save_dataframe(df_daytime_basket, 'df_daytime_basket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [47136, 2529, 8990]\n",
    "users = []\n",
    "daytime = 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot set a DataFrame with multiple columns to the single column distance\n"
     ]
    }
   ],
   "source": [
    "df, userb, prodb, basket, input_vector =  filter_dt_recommendation(product_list, users, daytime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>department</th>\n",
       "      <th>aisle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>Veggie Stix</td>\n",
       "      <td>snacks</td>\n",
       "      <td>chips pretzels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658</th>\n",
       "      <td>SeriousMilk Classic Milk Chocolate Bar</td>\n",
       "      <td>snacks</td>\n",
       "      <td>candy chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>Nuggets Milk Chocolate With Almond</td>\n",
       "      <td>snacks</td>\n",
       "      <td>candy chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>Macadamias, Sea Salt &amp; Cracked Pepper</td>\n",
       "      <td>snacks</td>\n",
       "      <td>nuts seeds dried fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10646</th>\n",
       "      <td>Pumpkin Pie Filling</td>\n",
       "      <td>pantry</td>\n",
       "      <td>baking ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12295</th>\n",
       "      <td>Superfruit Pomegranate Green Tea</td>\n",
       "      <td>beverages</td>\n",
       "      <td>tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18127</th>\n",
       "      <td>Cashew Milk Cappuccino Non-Dairy Frozen Dessert</td>\n",
       "      <td>frozen</td>\n",
       "      <td>ice cream ice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19052</th>\n",
       "      <td>Premium Ice</td>\n",
       "      <td>frozen</td>\n",
       "      <td>ice cream ice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23738</th>\n",
       "      <td>Cherry Flavor Primadophilus Kids Chewables</td>\n",
       "      <td>personal care</td>\n",
       "      <td>digestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23914</th>\n",
       "      <td>5 Symptom Digestive Relief</td>\n",
       "      <td>personal care</td>\n",
       "      <td>digestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27757</th>\n",
       "      <td>Creamy Gorg Blue-Veined Cheese Soft</td>\n",
       "      <td>dairy eggs</td>\n",
       "      <td>specialty cheeses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36659</th>\n",
       "      <td>Caribbean Rice Mix</td>\n",
       "      <td>dry goods pasta</td>\n",
       "      <td>instant foods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45774</th>\n",
       "      <td>Two Organic Cornmeal Pizza Crusts</td>\n",
       "      <td>deli</td>\n",
       "      <td>prepared meals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45932</th>\n",
       "      <td>Deviled Egg Tray</td>\n",
       "      <td>deli</td>\n",
       "      <td>prepared meals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47506</th>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>red wines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          product_name       department  \\\n",
       "1963                                       Veggie Stix           snacks   \n",
       "4658            SeriousMilk Classic Milk Chocolate Bar           snacks   \n",
       "5017                Nuggets Milk Chocolate With Almond           snacks   \n",
       "5924             Macadamias, Sea Salt & Cracked Pepper           snacks   \n",
       "10646                              Pumpkin Pie Filling           pantry   \n",
       "12295                 Superfruit Pomegranate Green Tea        beverages   \n",
       "18127  Cashew Milk Cappuccino Non-Dairy Frozen Dessert           frozen   \n",
       "19052                                      Premium Ice           frozen   \n",
       "23738       Cherry Flavor Primadophilus Kids Chewables    personal care   \n",
       "23914                       5 Symptom Digestive Relief    personal care   \n",
       "27757              Creamy Gorg Blue-Veined Cheese Soft       dairy eggs   \n",
       "36659                               Caribbean Rice Mix  dry goods pasta   \n",
       "45774                Two Organic Cornmeal Pizza Crusts             deli   \n",
       "45932                                 Deviled Egg Tray             deli   \n",
       "47506                               Cabernet Sauvignon          alcohol   \n",
       "\n",
       "                        aisle  \n",
       "1963           chips pretzels  \n",
       "4658          candy chocolate  \n",
       "5017          candy chocolate  \n",
       "5924   nuts seeds dried fruit  \n",
       "10646      baking ingredients  \n",
       "12295                     tea  \n",
       "18127           ice cream ice  \n",
       "19052           ice cream ice  \n",
       "23738               digestion  \n",
       "23914               digestion  \n",
       "27757       specialty cheeses  \n",
       "36659           instant foods  \n",
       "45774          prepared meals  \n",
       "45932          prepared meals  \n",
       "47506               red wines  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
