{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6517a63-6f41-469f-b298-28ce432d3a04",
   "metadata": {},
   "source": [
    "# Instacart Recommendation Engine\n",
    "\n",
    "This Jupyter notebook aims to build a recommendation engine using vector embeddings. The recommendation engine will utilize various features such as product names, shopping cart and user behaviour to generate relevant recommendations for users.\n",
    "\n",
    "The notebook is organized into the following sections:\n",
    "\n",
    "0. Flowchart\n",
    "1. Initial Setup: Loading necessary libraries and dependencies.\n",
    "2. Data Loading and Pre-processing: Loading and preparing the required data for the recommendation engine.\n",
    "3. Build Vector Embeddings: Creating vector embeddings for different features based on order carts.\n",
    "    - 3a. Build Product Embedding by Name: Vector representation for stem word.\n",
    "    - 3b. Build Product Embedding by Cart: Vector representation for product (id).\n",
    "    - 3c. Build Product Embedding by Department: Vector representation by Department.\n",
    "4. Concatenate Embeddings & Generate Representations: Combining the vector embeddings from the previous steps, generate representations for each order/user by averaging all product vectors in the order/ever ordered by user\n",
    "5. Vector Similarity Search (Output): Using the concatenated embeddings to perform similarity search and generate recommendations.\n",
    "6. Saving objects\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fd0d0-c9db-470d-aaaa-5bcadf588857",
   "metadata": {},
   "source": [
    "# Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071c43e-87c7-4ab4-a93a-c1a61284c380",
   "metadata": {},
   "source": [
    "![title](img/flowchart.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a46066-0260-4a19-a6fa-e62ff276d330",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769b04a-3625-4e3f-adee-549e70ec7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install annoy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9358af7-e09e-4541-a82d-65e6585d89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import flatten\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import seaborn as sns\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import zipfile as zp\n",
    "from art import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "from plotly.tools import FigureFactory as FF\n",
    "import plotly.express as px\n",
    "\n",
    "# from PyDictionary import PyDictionary P\n",
    "import random\n",
    "import time\n",
    "\n",
    "#import scikitplot as skplt\n",
    "\n",
    "#to enable the inline plotting\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36924b6-841f-415d-9bb6-5d65fa12eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to location of data (Instacart Market Basket Analysis Kaggle data)\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058fd26-6325-4145-9309-8bb117895ce4",
   "metadata": {},
   "source": [
    "## Data loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec69953-eb69-477d-bf3a-54001e3ec95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv files into corresponding dataframes\n",
    "# Load products, aisles and departments\n",
    "products = pd.read_csv(os.path.join(data_dir, \"products.csv\"))\n",
    "aisles = pd.read_csv(os.path.join(data_dir, \"aisles.csv\"))\n",
    "departments = pd.read_csv(os.path.join(data_dir, \"departments.csv\"))\n",
    "\n",
    "# Load orders dataset\n",
    "orders = pd.read_csv(os.path.join(data_dir, \"orders.csv\"))\n",
    "order_products_prior = pd.read_csv(os.path.join(data_dir, \"order_products__prior.csv\"))\n",
    "order_products_train = pd.read_csv(os.path.join(data_dir, \"order_products__train.csv\"))\n",
    "\n",
    "# Replacing numbers with their corresponding day of week\n",
    "days_of_week = {0: 'Saturday', \n",
    "                1: 'Sunday', \n",
    "                2: 'Monday',\n",
    "                3: 'Tuesday',\n",
    "                4: 'Wednesday',\n",
    "                5: 'Thursday',\n",
    "                6: 'Friday'}\n",
    "# Define the categories of days of week sorted normally from Saturday to Friday\n",
    "orders['order_dow'] = orders['order_dow'].replace(to_replace=days_of_week)\n",
    "orders['order_dow'] = pd.Categorical(orders['order_dow'], \n",
    "                                     ordered=True, \n",
    "                                     categories=list(days_of_week.values()))\n",
    "\n",
    "orders['daytime'] = orders['order_dow'].astype('str') + orders['order_hour_of_day'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a6da9-ed68-4d24-a086-95601bddd992",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Product name stemming and lemmatisation\n",
    "products['products_mod'] = products['product_name'].str.lower()\n",
    "# Clean special characters.\n",
    "products['products_mod'] = products['products_mod'].str.replace('\\W', ' ', regex=True)\n",
    "# Split products into terms: Tokenize.\n",
    "products['products_mod'] = products['products_mod'].str.split()\n",
    "# Merge the department and aisle names into the dataframe. \n",
    "products = pd.merge(products, departments, on=\"department_id\", how='outer')\n",
    "products = pd.merge(products, aisles, on=\"aisle_id\", how='outer')\n",
    "# Remove synonyms here in the list\n",
    "products['products_mod'] = products[['products_mod', 'aisle', 'department']].values.tolist()\n",
    "products['products_mod'] = products['products_mod'].apply(lambda x:list(flatten(x)))\n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "products['products_lemma'] = products['products_mod'].apply(lambda row:[lemma.lemmatize(item) for item in row])\n",
    "products['products_lemma'] = products['products_lemma'].apply(lambda row:[sno.stem(item) for item in row])\n",
    "\n",
    "prod_dict = dict(zip(products.product_id, products.product_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446c069-dce4-472e-bdba-e1a7fa9a1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add product information to order_products_prior\n",
    "order_products_prior = order_products_prior.merge(products[['product_id', 'department_id', 'aisle_id']], on='product_id')\n",
    "## Group all products for a single order into a list\n",
    "## This will be later used to generate embeddings\n",
    "order_baskets = order_products_prior.groupby('order_id')['product_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce101a1-314a-4e1f-bb2d-790081255934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate department and aisle baskets\n",
    "department_basket = order_products_prior.groupby('order_id')['department_id'].apply(list)\n",
    "aisle_basket = order_products_prior.groupby('order_id')['aisle_id'].apply(list)\n",
    "\n",
    "department_basket_unique = department_basket.apply(lambda x: list(set(x)))\n",
    "aisle_basket_unique = aisle_basket.apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec304f-0e20-4cb3-9829-408f48ccb5ff",
   "metadata": {},
   "source": [
    "## Word2Vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673230d-6803-4c95-86d0-b292eae7e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "\n",
    "# Vector size for word embeddings\n",
    "# Justification: A vector size of 8 strikes a balance between capturing semantic information and manageable dimensionality.\n",
    "WORD_VECTOR_SIZE = 8\n",
    "# Vector size for product embeddings\n",
    "# Justification: A vector size of 64 allows for capturing complex relationships and characteristics of the products.\n",
    "PRODUCT_VECTOR_SIZE = 64\n",
    "# Vector size for department embeddings\n",
    "# Justification: A vector size of 2 effectively represents the limited number of department categories with low computational complexity.\n",
    "DEPARTMENT_VECTOR_SIZE = 2\n",
    "\n",
    "# CPU Cores\n",
    "WORKER_COUNT = os.cpu_count()\n",
    "# Minimum frequency before dropping set to 1 to include each and every product, even if it was purchased once\n",
    "MIN_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c3898-59c5-496c-96f8-36eec504298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the `Word2Vec` model based on product lemma\n",
    "\n",
    "# Defining the maximun window\n",
    "window_max = max(products['products_lemma'].apply(lambda x:len(x)))\n",
    "\n",
    "# size=20: In order to make `Word2Vec` a little bit quicker and for memory efficiency we're going to use 20 dimensions.\n",
    "# window=49: In order to make sure all words are used in training the model, we're going to set a large.\n",
    "w2vec_model = Word2Vec(list(products['products_lemma']), vector_size=WORD_VECTOR_SIZE, window=window_max,\n",
    "                       min_count=MIN_COUNT, workers=WORKER_COUNT)\n",
    "\n",
    "### Vector calculation for products\n",
    "# Loop through each product and obtain the average of each string that makes a product.\n",
    "# This will be the vector representation of the product.\n",
    "# The vector representation of the product will be used to calculate the similarity between products.\n",
    "# The similarity between products will be used to recommend products to the user.\n",
    "\n",
    "# Loop through each word in the product name to generate the vector.\n",
    "prods_w2v = dict()\n",
    "for row, product in tqdm(products.iterrows()):\n",
    "    word_vector = list()\n",
    "    for word in product['products_lemma']:\n",
    "        word_vector.append(w2vec_model.wv[word])\n",
    "\n",
    "    prods_w2v[product['product_id']] = np.average(word_vector, axis=0)\n",
    "\n",
    "# Save vector values in list form to the dataframe.\n",
    "products['vectors_word'] = prods_w2v.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ca3dc-2f64-406d-ad8b-2d60cee4e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Training the `Word2Vec` model based on cart\n",
    "# products which are ordered together will be closer in vector space\n",
    "\n",
    "# Define maximum window for longest order cart\n",
    "window_max = order_baskets.apply(len).max()\n",
    "\n",
    "# w2vec model\n",
    "w2vec_model = Word2Vec(list(order_baskets), vector_size=PRODUCT_VECTOR_SIZE, window=window_max,\n",
    "                       min_count=MIN_COUNT, workers=WORKER_COUNT)\n",
    "\n",
    "# get vectors for each product\n",
    "products['vectors_product'] = products.product_id.apply(lambda x: w2vec_model.wv[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc20c7-6f4f-4b8c-993c-0e845c74bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # Define maximum window for longest order cart\n",
    "window_max = department_basket.apply(len).max()\n",
    "\n",
    "# w2vec model\n",
    "w2vec_model = Word2Vec(list(department_basket), vector_size=DEPARTMENT_VECTOR_SIZE, window=window_max, \n",
    "                       min_count=MIN_COUNT, workers=WORKER_COUNT)\n",
    "\n",
    "# get vectors for each product\n",
    "products['vectors_dept'] = products.department_id.apply(lambda x: w2vec_model.wv[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad8324-4411-4352-ae66-ed0a59990b21",
   "metadata": {},
   "source": [
    "## Vector concatenation and order/user representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ad8ad-f1a4-4c1c-bc1d-f0215b959f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using `annoy` model to calculate the similarity between products\n",
    "def annoy_build(df, id, vector_size, metric='euclidean'):\n",
    "    trees = 10\n",
    "    m = AnnoyIndex(vector_size, metric=metric) \n",
    "    m.set_seed(42)\n",
    "    for _, row in df.iterrows():\n",
    "        m.add_item(row[id], row['vectors'])\n",
    "    m.build(trees)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bebd9d-5dab-4ae3-a8be-9c5ee6d53b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate all vectors together to form a single representation of product embedding\n",
    "products['vectors'] = products.apply(lambda x: [*x['vectors_word'], *x['vectors_product'], *x['vectors_dept']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81458a02-82de-48cb-9e70-02f3daabc943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Train `annoy` for `product` dataset\n",
    "### Annoy object can be used to identify similar products for a given product embedding\n",
    "p = annoy_build(products, 'product_id', len(products['vectors'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3befedc-0165-42c6-84d6-b3f109d5e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Train `annoy` for `orders` dataset\n",
    "### Annoy object can be used to identify similar orders for a given embedding\n",
    "order_w2v = dict()\n",
    "for index, row in tqdm(order_baskets.items()):\n",
    "    word_vector = list()\n",
    "    for item_id in row:\n",
    "        word_vector.append(p.get_item_vector(item_id))\n",
    "    order_w2v[index] = np.average(word_vector, axis=0)\n",
    "\n",
    "df_order_baskets = pd.DataFrame({'order_id': order_baskets.index, 'product_id': order_baskets.values})\n",
    "df_order_baskets['vectors'] = order_w2v.values()\n",
    "\n",
    "# Specify the metric to be used for computing distances. \n",
    "b = annoy_build(df_order_baskets, 'order_id', len(df_order_baskets['vectors'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b61891-c04d-4053-8927-88a1a27184ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def func_avg(x):\n",
    "    return np.average(x, axis=0)\n",
    "\n",
    "### Train `annoy` for `orders` dataset\n",
    "### Annoy object can be used to identify similar users for a given embedding\n",
    "\n",
    "user_basket = pd.merge(df_order_baskets, orders, on=\"order_id\", how='inner')\n",
    "\n",
    "df_user_basket = user_basket[['user_id', 'vectors', 'product_id']]\n",
    "df_user_basket = df_user_basket.groupby('user_id').agg(list)\n",
    "df_user_basket['vectors'] = df_user_basket['vectors'].agg(func_avg).apply(tuple)\n",
    "df_user_basket['product_id'] = df_user_basket['product_id'].agg(lambda x: list(set([item for sublist in x for item in sublist])))\n",
    "df_user_basket = df_user_basket.reset_index()\n",
    "\n",
    "# Specify the metric to be used for computing distances. \n",
    "u = annoy_build(df_user_basket, 'user_id', len(df_user_basket.vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404b997-556c-4e91-904c-01bd3996ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# ### Train `annoy` for `daytime` data\n",
    "# daytime_basket = pd.merge(df_order_baskets, orders, on='order_id', how='inner')\n",
    "# daytime_basket = daytime_basket.groupby('daytime').apply(lambda x: [list(x['vectors']), list(x['product_id'])]).apply(pd.Series)\n",
    "# daytime_basket.columns =['vectors','product_id']\n",
    "# daytime_basket['vectors'] = daytime_basket['vectors'].apply(lambda x: tuple(np.average(x, axis=0)))\n",
    "# daytime_basket['product_id'] = daytime_basket['product_id'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "# daytime_basket['product_id'] = daytime_basket['product_id'].apply(lambda x: list(set(x)))\n",
    "# df_daytime_basket = daytime_basket.reset_index().reset_index().rename(columns={'index':'daytime_id'})\n",
    "# # Specify the metric to be used for computing distances. \n",
    "# d = annoy_build(df_daytime_basket, 'daytime_id', len(df_daytime_basket.vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90308071-ee95-4712-bfd0-cc3c0d3c35f4",
   "metadata": {},
   "source": [
    "## Testing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e2293-2bcc-4637-94cd-c6c3ac2bbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs can be of multiple types\n",
    "# 1. Only products input - Get centroid vector of products, then find carts that are similar and rank products\n",
    "# 2. Only user_ids input - Get centroid vector of user_ids, then find carts that are similar and rank products\n",
    "# 3. Both products and user_ids as input - Get products and user centroid, then find products by product and dot product with user centroid to find best products\n",
    "# 4. No input - Find products based on daytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b03861-754b-4bc2-9ba8-51fe4240a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [47136, 2529, 8990]\n",
    "user_list = [1,2,3]\n",
    "daytime = 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcb241-dc73-459c-9722-588ccbbffc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_by_euclidean(df, vector):\n",
    "    df['distance'] = df['vectors'].apply(lambda x: np.linalg.norm(x - vector))\n",
    "    df = df.sort_values('distance', ascending=False)\n",
    "    return df\n",
    "\n",
    "def rank_by_dot_product(df, vector):\n",
    "    df['dot_prod'] = df['vectors'].apply(lambda x: np.dot(x, vector))\n",
    "    df = df.sort_values('dot_prod', ascending=False)\n",
    "    return df\n",
    "\n",
    "def compose_basket_by_cart(product_vector, input = None, n_items = 15, method='euclidean', n_neighbours = 100):\n",
    "    order_list = b.get_nns_by_vector(product_vector, n_items)\n",
    "    fpl = []\n",
    "    for order in order_list:\n",
    "        fpl = fpl + order_baskets[order]\n",
    "    \n",
    "    fpl = list(set(fpl))\n",
    "\n",
    "    sel_df = pd.DataFrame({\"product_id\": fpl}).merge(products, on='product_id', how='inner')\n",
    "    if method == 'euclidean':\n",
    "        sel_df = rank_by_euclidean(sel_df, product_vector)\n",
    "    else:  \n",
    "        sel_df = dot_prod_rank(sel_df, product_vector)\n",
    "    if input is not None:\n",
    "        sel_df = sel_df[~sel_df.product_id.isin(input)]\n",
    "    sel_df = sel_df[['product_name', 'department', 'aisle']].head(n_items).reset_index(drop=True)\n",
    "    return sel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eca811-ead2-436b-a69a-4b0d89c02082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(vector_list):\n",
    "    return np.average(vector_list, axis=0)\n",
    "\n",
    "def get_centroid_by_annoy_obj(obj_ann, obj_list):\n",
    "    w2v_list = []\n",
    "    for obj_id in obj_list:\n",
    "        w2v_list.append(obj_ann.get_item_vector(obj_id))\n",
    "    return get_centroid(w2v_list)\n",
    "\n",
    "def get_basket_by_product_list(product_list, n_items = 15):\n",
    "    selected = products[products.product_id.isin(product_list)]\n",
    "    prod_vector = get_centroid(selected.vectors.tolist())\n",
    "    return compose_basket_by_cart(prod_vector, input=selected.product_name.tolist(), n_items=n_items)\n",
    "\n",
    "def get_basket_by_user_list(user_list, n_items = 15):\n",
    "    user_vector = get_centroid_by_annoy_obj(u, user_list)\n",
    "    return compose_basket_by_cart(user_vector, n_items=n_items)\n",
    "\n",
    "def get_basket_by_user_product(product_list, user_list, n_items = 15):\n",
    "    # Get product vector\n",
    "    selected = products[products.product_id.isin(product_list)]\n",
    "    prod_vector = get_centroid(selected.vectors.tolist())\n",
    "    # Get 1000 nearest products\n",
    "    similar_prod_1000 = p.get_nns_by_vector(prod_vector, 1000)\n",
    "    sel_df = pd.DataFrame({\"product_id\": similar_prod_1000}).merge(products, on='product_id', how='inner')\n",
    "\n",
    "    # Get user vector\n",
    "    user_vector = get_centroid_by_annoy_obj(u, user_list)\n",
    "    # Rank 1000 products by aggregated user vector\n",
    "    sel_df = rank_by_euclidean(sel_df, user_vector)\n",
    "    # Remove input products\n",
    "    sel_df = sel_df[~sel_df.product_id.isin(selected.product_name.tolist())]\n",
    "    # Return top n\n",
    "    sel_df = sel_df[['product_name', 'department', 'aisle']].head(n_items).reset_index(drop=True)\n",
    "    return sel_df\n",
    "\n",
    "def get_basket_by_daytime(daytime_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35411c4b-e495-49ee-9505-bc34c32a1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_basket_by_user_product(product_list, user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462f215-5c75-4bab-a1c5-2e7592fd2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_basket_by_product_list(product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb3431-3e47-4d3c-a7c8-6509cc1f281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_basket_by_user_list(user_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bacc0-3154-4013-866a-e46048ccda29",
   "metadata": {},
   "source": [
    "## Save objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab95cfe-fb27-4e48-837b-8123eec5b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### This section will save resources\n",
    "### These resources can later be used by an app to run the engine\n",
    "\n",
    "save_path = os.path.join(os.path.dirname(os.getcwd()), 'res')\n",
    "\n",
    "def save_annoy(obj, n):\n",
    "    path = os.path.join(save_path, n + \".ann\")\n",
    "    obj.save(path)\n",
    "\n",
    "## Save annoy objects\n",
    "save_annoy(p, \"product\")\n",
    "save_annoy(u, \"user\")\n",
    "save_annoy(b, \"basket\")\n",
    "#save_annoy(d, \"daytime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf4b79-5419-4108-8e39-c6c6b6c7ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Save dataframes to avoid pre-processing\n",
    "def save_dataframe(obj, n):\n",
    "    path = os.path.join(save_path, n + \".pkl\")\n",
    "    obj.to_pickle(path)\n",
    "    print(path, \"saved !\")\n",
    "\n",
    "save_dataframe(products, 'products')\n",
    "save_dataframe(order_baskets, 'order_baskets')\n",
    "#save_dataframe(df_daytime_basket, 'df_daytime_basket')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
